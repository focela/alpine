#!/command/with-contenv bash
#-----------------------------------------------------------------------------
# Logging Configuration Script
#
# Purpose: Configure log rotation and log shipping for container environments
# Context: Runs in s6-overlay environment as part of container initialization
# Note: Runs after monitoring setup in the cont-init.d sequence (05-logging)
#
# Features:
# - Log rotation with multiple compression formats (bzip2, gzip, zstd, none)
# - Fluent-bit log shipping with multiple backends (Loki, FluentD, NULL)
# - Architecture and OS compatibility validation
# - Dynamic log source configuration via LOGSHIP_* environment variables
# - Multiple Fluent-bit modes (normal/client, proxy/forward)
# - Integration with Zabbix monitoring
#
# Environment Variables:
# - CONTAINER_ENABLE_LOGROTATE: Enable/disable log rotation
# - LOGROTATE_COMPRESSION_TYPE: Compression method (bzip2/gzip/zstd/none)
# - LOGROTATE_COMPRESSION_VALUE: Compression level (1-9)
# - LOGROTATE_RETAIN_DAYS: Number of days to retain rotated logs
# - CONTAINER_ENABLE_LOGSHIPPING: Enable/disable log shipping
# - CONTAINER_LOGSHIPPING_BACKEND: Log shipping backend (fluent-bit)
# - FLUENTBIT_SETUP_TYPE: Configuration mode (auto/manual)
# - FLUENTBIT_MODE: Operation mode (normal/proxy/forward)
# - FLUENTBIT_OUTPUT: Output destination (loki/fluentd/forward/null)
# - LOGSHIP_*: Dynamic log source definitions
# - Plus many other Fluent-bit specific configuration variables
#-----------------------------------------------------------------------------



#-----------------------------------------------------------------------------
# INITIALIZATION
#-----------------------------------------------------------------------------
# Load container functions library and initialize process
source /assets/functions/00-container
output_off
prepare_service
PROCESS_NAME="logging"

#-----------------------------------------------------------------------------
# LOG ROTATION CONFIGURATION
#-----------------------------------------------------------------------------
# Configure system log rotation if enabled
if var_true "${CONTAINER_ENABLE_LOGROTATE}" ; then
  print_debug "Enabling log rotation"

  # Configure compression based on specified type
  case "${LOGROTATE_COMPRESSION_TYPE,,}" in
    bz* )
      # Bzip2 compression: Higher compression ratio, slower processing
      logrotate_compression=$(cat<<EOF
compress
compresscmd $(which bzip2)
compressext .bz2
compressoptions -${LOGROTATE_COMPRESSION_VALUE} ${LOGROTATE_COMPRESSION_EXTRA_PARAMETERS}
EOF
      )
    ;;
    gz* )
      # Gzip compression: Balanced compression and speed
      logrotate_compression=$(cat<<EOF
compress
compresscmd $(which gzip)
compressext .gz
compressoptions -${LOGROTATE_COMPRESSION_VALUE} ${LOGROTATE_COMPRESSION_EXTRA_PARAMETERS}
EOF
      )
    ;;
    none )
      # No compression: Fastest processing, largest files
      logrotate_compression=""
    ;;
    zs* )
      # Zstd compression: Modern algorithm with good compression and speed
      logrotate_compression=$(cat<<EOF
compress
compresscmd $(which zstd)
compressext .zst
compressoptions -${LOGROTATE_COMPRESSION_VALUE} ${LOGROTATE_COMPRESSION_EXTRA_PARAMETERS}
EOF
      )
    ;;
  esac

  # Generate main logrotate configuration file
  cat <<EOF > /etc/logrotate.conf
daily
rotate ${LOGROTATE_RETAIN_DAYS}
copytruncate
dateext
nomail
notifempty
${logrotate_compression}
include /etc/logrotate.d
EOF

  # Set appropriate permissions and create scheduled execution
  chmod 0744 /etc/logrotate.conf
  mkdir -p "${CONTAINER_SCHEDULING_LOCATION}"

  # Schedule logrotate to run daily at 23:59 (1 minute before midnight)
  cat <<EOF > "${CONTAINER_SCHEDULING_LOCATION}"/logrotate
# Hardcoded in image in /etc/cont-init.d/$(basename "$0")
# Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

59 23 * * * logrotate -f /etc/logrotate.conf >/dev/null 2>&1
EOF
fi

#-----------------------------------------------------------------------------
# LOG SHIPPING CONFIGURATION
#-----------------------------------------------------------------------------
# Configure log shipping backend if enabled
if var_false "${CONTAINER_ENABLE_LOGSHIPPING}" ; then
  # Disable log shipping service if not enabled
  service_stop "$(basename "$0")"
else
  case "${CONTAINER_LOGSHIPPING_BACKEND,,}" in
    "fluent-bit" | "fluentbit" )
      #-------------------------------------------------------------------------
      # ARCHITECTURE AND OS COMPATIBILITY VALIDATION
      #-------------------------------------------------------------------------
      # Validate OS and architecture compatibility for Fluent-bit
      os=$(cat /etc/os-release |grep ^ID= | cut -d = -f2)
      case ${os,,} in
        "alpine" )
          # Alpine Linux: Check architecture and version compatibility
          archit="$(apk --print-arch)"
          case "$archit" in
            x86_64)
              # Alpine x86_64: Require version 3.11 or newer
              osver=$(cat /etc/os-release | grep VERSION_ID | cut -d = -f 2 | cut -d . -f 2 | cut -d _ -f 1)
              if [ "${osver}" -ge 11 ] || [ "$osver" = "edge" ] || [ "$osver" = "17*" ]; then
                : # Version check passed
              else
                print_error "Sorry this functionality is not available on < Alpine 3.11 releases"
                service_stop "$(basename "$0")"
                liftoff
                exit 0
              fi
            ;;
            *)
              # Unsupported architecture for Alpine
              print_error "Sorry this functionality is not available on ${archit} architecture"
              service_stop "$(basename "$0")"
              liftoff
              exit
            ;;
          esac
        ;;
        "debian" | "ubuntu" )
          # Debian/Ubuntu: Check architecture compatibility
          archit=$(dpkg --print-architecture) && \
          case "$archit" in \
            amd64)
              : # Architecture check passed
            ;;
            *)
              # Unsupported architecture for Debian/Ubuntu
              print_error "Sorry this functionality is not available on ${archit} architecture"
              service_stop "$(basename "$0")"
              liftoff
              exit
            ;;
          esac
        ;;
      esac

      #-------------------------------------------------------------------------
      # FLUENT-BIT AUTO CONFIGURATION
      #-------------------------------------------------------------------------
      # Generate Fluent-bit configuration automatically if enabled
      if [ "${FLUENTBIT_SETUP_TYPE,,}" = "auto" ] ; then
        print_debug "[logship] Configuring Fluent-bit agent"

        # Convert boolean environment variables to on/off format for Fluent-bit
        truefalse_onoff FLUENTBIT_ENABLE_HTTP_SERVER
        truefalse_onoff FLUENTBIT_ENABLE_STORAGE_METRICS
        truefalse_onoff FLUENTBIT_STORAGE_CHECKSUM

        # Create required directory structure
        mkdir -p "${FLUENTBIT_STORAGE_PATH}"
        mkdir -p "${FLUENTBIT_LOG_PATH}"
        mkdir -p /etc/fluent-bit/conf.d

        # Create dummy configuration to prevent Fluent-bit startup failures
        # This ensures Fluent-bit always has at least one valid input/output pair
        cat <<EOF > /etc/fluent-bit/conf.d/do_not_delete.conf
# Don't delete this configuration file otherwise execution of fluent-bit will fail. It will not affect operation of your system or impact resources
[INPUT]
    Name   dummy
    Tag    ignore

[FILTER]
    Name grep
    Match ignore
    regex ignore ignore

[OUTPUT]
    Name   NULL
    Match  ignore
EOF

        # Process additional custom parsers if they exist
        if [ "$(ls -A /etc/fluent-bit/parsers.d/)" ]; then
          shopt -s nullglob
          for custom_parser in /etc/fluent-bit/parsers.d/*.conf ; do
            print_debug "[logship] Found additional parser for '$(echo "${custom_parser,,}" | sed "s|.conf||g")'"
            additional_parsers=$(echo "${additional_parsers}" ; cat<<EOF
    parsers_file ${custom_parser}
EOF
            )
          done
          shopt -u nullglob
        fi

        # Generate main Fluent-bit configuration file
        cat <<EOF > /etc/fluent-bit/fluent-bit.conf
## Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

@INCLUDE conf.d/*.conf
[SERVICE]
    daemon       Off
    flush        ${FLUENTBIT_FLUSH_SECONDS}
    grace        ${FLUENTBIT_GRACE_SECONDS}
    http_listen  ${FLUENTBIT_HTTP_LISTEN_IP}
    http_port    ${FLUENTBIT_HTTP_LISTEN_PORT}
    http_server  ${FLUENTBIT_ENABLE_HTTP_SERVER}
    log_file     ${FLUENTBIT_LOG_PATH}/${FLUENTBIT_LOG_FILE}
    log_level    ${FLUENTBIT_LOG_LEVEL}
    plugins_file ${FLUENTBIT_CONFIG_PLUGINS}
    storage.backlog.mem_limit ${FLUENTBIT_STORAGE_BACKLOG_LIMIT}
    storage.checksum ${FLUENTBIT_STORAGE_CHECKSUM}
    storage.metrics ${FLUENTBIT_ENABLE_STORAGE_METRICS}
    storage.path ${FLUENTBIT_STORAGE_PATH}
    storage.sync ${FLUENTBIT_STORAGE_SYNC}
    parsers_file ${FLUENTBIT_CONFIG_PARSERS}
${additional_parsers}
EOF

        #-----------------------------------------------------------------------
        # FLUENT-BIT INPUT PLUGIN CONFIGURATION
        #-----------------------------------------------------------------------
        # Configure input plugins based on operation mode
        case "${FLUENTBIT_MODE,,}" in
          "normal" )
            print_debug "[logship] Configuring Fluent-Bit for Normal/Client mode"

            # Configure tail input plugin options
            if var_true "${FLUENTBIT_TAIL_KEY_PATH_ENABLE}" ; then
              tail_key_path="    Path_Key          ${FLUENTBIT_TAIL_KEY_PATH}"
            fi

            if var_true "${FLUENTBIT_TAIL_KEY_OFFSET_ENABLE}" ; then
              tail_key_offset="    Offset_Key        ${FLUENTBIT_TAIL_KEY_OFFSET}"
            fi

            if [ -n "${FLUENTBIT_TAIL_IGNORE_OLDER}" ] ; then
              tail_ignore_older="    Ignore_Older      ${FLUENTBIT_TAIL_IGNORE_OLDER}"
            fi

            truefalse_onoff FLUENTBIT_TAIL_SKIP_EMPTY_LINES
            truefalse_onoff FLUENTBIT_TAIL_SKIP_LONG_LINES

            #-------------------------------------------------------------------
            # DYNAMIC LOG SOURCE PROCESSING
            #-------------------------------------------------------------------
            # Process LOGSHIP_* environment variables for dynamic log configuration
            logshipenv=$(mktemp)
            set -o posix; set -f ; set | grep -E '^LOGSHIP_'| sed "s|LOGSHIP_||g" > "${logshipenv}"

            # Process each LOGSHIP_* variable
            while IFS= read -r logship_entry; do
              logship_title="$(echo "${logship_entry}" | cut -d = -f1 | tr '[:upper:]' '[:lower:]' )"
              logship_value="$(echo "${logship_entry}" | cut -d = -f2 )"
              logship_value="$(echo "${logship_value:1:-1}")"

              if var_false "${logship_value}" ; then
                # Create null configuration for disabled log sources
                print_debug "[logship] Disabling ${logship_title} Log Shipping"
                cat <<EOF > "/etc/fluent-bit/conf.d/in_tail_${logship_title,,}.conf"
# Nulled Log Monitoring of ${logship_title}  generated by Environment Variable defined in Image build, or Runtime argument
# Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')
EOF
                # Update logrotate configuration to skip this log
                if [ -f "/assets/logrotate/${logship_title,,}" ] ; then
                  print_debug "[logship] Setting Logrotate value to skip for /assets/logrotate/${logship_title,,}"
                  sed -i "# logship: .*|# logship: ignore"
                fi
              else
                # Create active tail configuration for enabled log sources
                print_debug "[logship] Adding ${logship_title} with to be parsed by ${CONTAINER_LOGSHIPPING_BACKEND}"

                # Configure database tracking if enabled
                if var_true "${FLUENTBIT_TAIL_DB_ENABLE}" ; then
                  tail_db=$(cat<<EOF
    DB                $(dirname ${logship_value})/.$(basename ${logship_value}).db
    DB.sync           ${FLUENTBIT_TAIL_DB_SYNC}
    DB.locking        ${FLUENTBIT_TAIL_DB_LOCK}
    DB.journal_mode   ${FLUENTBIT_TAIL_DB_JOURNAL_MODE}
EOF
                  )
                fi

                # Generate tail input configuration for this log source
                cat <<EOF > "/etc/fluent-bit/conf.d/in_tail_${logship_title,,}.conf"
# Log File Monitoring created automatically generated by Environment Variable defined in Image build, or Runtime argument
# Entered Value: ${logship_value}
# Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

[INPUT]
    Name              tail
    Path              ${logship_value}
    Tag               ${logship_title,,}
    Buffer_Chunk_Size ${FLUENTBIT_TAIL_BUFFER_CHUNK_SIZE}
    Buffer_Max_Size   ${FLUENTBIT_TAIL_BUFFER_MAX_SIZE}
    Read_from_Head    ${FLUENTBIT_TAIL_READ_FROM_HEAD}
    Skip_Empty_Lines  ${FLUENTBIT_TAIL_SKIP_EMPTY_LINES}
    Skip_Long_Lines   ${FLUENTBIT_TAIL_SKIP_LONG_LINES}
${tail_key_path}
${tail_db}
${tail_ignore_older}
${tail_key_offset}

[FILTER]
    Name record_modifier
    Match ${logship_title,,}
    Record hostname $(hostname)
    Record container_name ${CONTAINER_NAME}
    Record product ${logship_title,,}

EOF
              fi
            done < "${logshipenv}"
            rm -rf "$logshipenv"
            set +f
            unset logshipenv logship_entry logship_value logship_title db
            unset "${!LOGSHIP_@}"

            #-------------------------------------------------------------------
            # DEFAULT LOG SOURCE PROCESSING
            #-------------------------------------------------------------------
            # Process default log sources from /assets/defaults/*
            for d in /assets/defaults/* ; do
              if [ "$d" != "/assets/defaults/00-container" ] ; then
                # shellcheck source=/assets/defaults/
                source "$d"
                logshipenv=$(mktemp)
                set -o posix; set -f ; set | grep -E '^LOGSHIP_'| sed "s|LOGSHIP_||g" > "${logshipenv}"

                # Process each default LOGSHIP_* variable
                while IFS= read -r logship_entry; do
                  logship_title="$(echo "${logship_entry}" | cut -d = -f1 | tr '[:upper:]' '[:lower:]')"
                  logship_value="$(echo "${logship_entry}" | cut -d = -f2 )"
                  logship_value="$(echo "${logship_value:1:-1}")"

                  # Configure database tracking if enabled
                  if var_true "${FLUENTBIT_TAIL_DB_ENABLE}" ; then
                    tail_db=$(cat<<EOF
    DB                $(dirname ${logship_value})/.$(basename ${logship_value}).db
    DB.sync           ${FLUENTBIT_TAIL_DB_SYNC}
    DB.locking        ${FLUENTBIT_TAIL_DB_LOCK}
    DB.journal_mode   ${FLUENTBIT_TAIL_DB_JOURNAL_MODE}
EOF
                    )
                  fi

                  # Only create configuration if it doesn't already exist
                  if [ ! -f "/etc/fluent-bit/conf.d/in_tail_${logship_title,,}.conf" ] ; then
                    print_debug "[logship] Adding ${logship_title} with to parsed by ${CONTAINER_LOGSHIPPING_BACKEND}"
                    cat <<EOF > "/etc/fluent-bit/conf.d/in_tail_${logship_title,,}.conf"
# Log File Shipping created automatically generated by reading through defaults in /assets/defaults/*
# Entered Value: ${logship_value}
# Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

[INPUT]
    Name              tail
    Path              ${logship_value}
    Tag               ${CONTAINER_NAME}_${logship_title,,}
    Buffer_Chunk_Size ${FLUENTBIT_TAIL_BUFFER_CHUNK_SIZE}
    Buffer_Max_Size   ${FLUENTBIT_TAIL_BUFFER_MAX_SIZE}
    Read_from_Head    ${FLUENTBIT_TAIL_READ_FROM_HEAD}
    Skip_Empty_Lines  ${FLUENTBIT_TAIL_SKIP_EMPTY_LINES}
    Skip_Long_Lines   ${FLUENTBIT_TAIL_SKIP_LONG_LINES}
${tail_key_path}
${tail_db}
${tail_ignore_older}
${tail_key_offset}
EOF
                  else
                    print_debug "[logship] Skipping adding ${logship_title} as it already exists"
                  fi
                done < "${logshipenv}"
                rm -rf "$logshipenv"
                set +f
                unset logshipenv logship_entry logship_value logship_title
                unset "${!LOGSHIP_@}"
              fi
            done
          ;;

          "proxy" | "forward" )
            # Configure Fluent-bit as a proxy/forwarder (receives logs from other Fluent-bit instances)
            print_debug "[logship] Configuring Fluent-Bit for Proxy/Forwarding Mode"
            cat <<EOF > /etc/fluent-bit/conf.d/in_forward.conf
[INPUT]
    Name forward
    Listen 0.0.0.0
    Port ${FLUENTBIT_FORWARD_PORT}
    Buffer_Chunk_Size ${FLUENTBIT_FORWARD_BUFFER_CHUNK_SIZE}
    Buffer_Max_Size   ${FLUENTBIT_FORWARD_BUFFER_MAX_SIZE}
EOF
          ;;
        esac

        #-----------------------------------------------------------------------
        # FLUENT-BIT OUTPUT PLUGIN CONFIGURATION
        #-----------------------------------------------------------------------
        # Configure output plugins based on specified backend
        case "${FLUENTBIT_OUTPUT,,}" in
          "loki" )
            # Grafana Loki output configuration
            transform_file_var \
                            FLUENTBIT_OUTPUT_LOKI_HOST \
                            FLUENTBIT_OUTPUT_LOKI_PORT \
                            FLUENTBIT_OUTPUT_LOKI_USER \
                            FLUENTBIT_OUTPUT_LOKI_PASS \
                            FLUENTBIT_OUTPUT_LOKI_TENANT_ID

            # Configure authentication if credentials provided
            if [ -n "${FLUENTBIT_OUTPUT_LOKI_USER}" ] ; then
              loki_user="    http_user              ${FLUENTBIT_OUTPUT_LOKI_USER}"
            fi

            if [ -n "${FLUENTBIT_OUTPUT_LOKI_PASS}" ] ; then
              loki_pass="    http_passwd            ${FLUENTBIT_OUTPUT_LOKI_PASS}"
            fi

            # Configure multi-tenancy if tenant ID provided
            if [ -n "${FLUENTBIT_OUTPUT_LOKI_TENANT_ID}" ] ; then
              loki_tenant_id="    tenant_id              ${FLUENTBIT_OUTPUT_LOKI_TENANT_ID}"
            fi

            truefalse_onoff FLUENTBIT_OUTPUT_LOKI_TLS
            truefalse_onoff FLUENTBIT_OUTPUT_LOKI_TLS_VERIFY
            truefalse_onoff FLUENTBIT_OUTPUT_LOKI_COMPRESS_GZIP

            cat <<EOF > /etc/fluent-bit/conf.d/out_loki.conf
## Auto generated LOKI Output plugin for Fluent Bit
## Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

[OUTPUT]
    name                   loki
    match                  *
    host                   ${FLUENTBIT_OUTPUT_LOKI_HOST}
    port                   ${FLUENTBIT_OUTPUT_LOKI_PORT}
    tls                    ${FLUENTBIT_OUTPUT_LOKI_TLS,,}
    tls.verify             ${FLUENTBIT_OUTPUT_LOKI_TLS_VERIFY,,}
    compress               ${FLUENTBIT_OUTPUT_LOKI_COMPRESS_GZIP,,}
    labels                 logshipper=${CONTAINER_NAME}
    Label_keys             \$hostname,\$container_name,\$product
${loki_user}
${loki_pass}
${loki_tenant_id}

EOF
          ;;

          "fluentd" | "forward" )
            # FluentD forward output configuration
            truefalse_onoff FLUENTBIT_OUTPUT_FORWARD_TLS
            truefalse_onoff FLUENTBIT_OUTPUT_FORWARD_TLS_VERIFY

            # Configure shared secret if provided
            if [ ! -z "${FLUENTBIT_OUTPUT_FORWARD_SECRET}" ] ; then
              forward_secret="    Shared_Key    ${FLUENTBIT_OUTPUT_FORWARD_SECRET}"
            fi

            transform_file_var FLUENTBIT_OUTPUT_FORWARD_HOST
            cat <<EOF > /etc/fluent-bit/conf.d/out_forward.conf
## Auto generated FluentD Forward Output plugin for Fluent Bit
## Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

[OUTPUT]
    Name          forward
    Match         *
    Host          ${FLUENTBIT_OUTPUT_FORWARD_HOST}
    Port          ${FLUENTBIT_FORWARD_PORT}
    Self_Hostname ${CONTAINER_NAME}
    tls           ${FLUENTBIT_OUTPUT_FORWARD_TLS,,}
    tls.verify    ${FLUENTBIT_OUTPUT_FORWARD_TLS_VERIFY,,}
${forward_secret}
EOF
          ;;

          "null" )
            # Null output (discard logs - useful for testing)
            cat <<EOF > /etc/fluent-bit/conf.d/out_null.conf
## Auto generated NULL Output plugin for Fluent Bit
## Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')

[OUTPUT]
    Name null
    Match *
EOF
          ;;
        esac

      else
        # Manual configuration mode - user provides their own config files
        print_notice "[logship] Not auto configuring Fluent-Bit. Drop configuration files in /etc/fluent-bit/conf.d"
        cat <<EOF > /etc/fluent-bit/fluent-bit.conf
## This configuration file allows you to put your own configuration in /etc/fluent-bit/conf.d - Don't delete or it will fail :)
## Generated on $(TZ=${TIMEZONE} date +'%Y-%m-%d %H:%M:%S %Z')
@INCLUDE conf.d/*.conf
EOF
      fi

      #-------------------------------------------------------------------------
      # FINAL FLUENT-BIT SETUP
      #-------------------------------------------------------------------------
      # Configure log rotation for Fluent-bit logs
      create_logrotate fluentbit "${FLUENTBIT_LOG_PATH}"/"${FLUENTBIT_LOG_FILE}"

      # Integrate with Zabbix monitoring if both are enabled
      if var_true "${CONTAINER_ENABLE_MONITORING}" && [ "${CONTAINER_MONITORING_BACKEND,,}" = "zabbix" ]; then
        cat <<EOF > "${ZABBIX_CONFIG_PATH}"/"${ZABBIX_CONFIG_FILE}".d/focela-fluentbit.conf
# Zabbix Fluentbit Configuration - Automatically generated based on container startup options
# Find Companion Zabbix Server Templates at https://github.com/focela/docker-alpine or https://github.com/focela/docker-debian
# Autoregister=fluentbit
EOF
      fi

      print_notice "Container configured to ship logs via '${CONTAINER_LOGSHIPPING_BACKEND}'"
    ;;

    *)
      # Handle unknown log shipping backends
      print_error "[logship] Unknown Log Shipping Backend"
      exit 1
    ;;
  esac
fi

#-----------------------------------------------------------------------------
# FINAL INITIALIZATION
#-----------------------------------------------------------------------------
# Mark initialization complete and restore output
liftoff
output_on
